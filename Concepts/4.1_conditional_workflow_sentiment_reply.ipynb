{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c865c7e8",
   "metadata": {},
   "source": [
    "# 🎯 Problem Statement: Intelligent Review Response Generation using LangGraph Conditional Workflow\n",
    "\n",
    "## 🧩 Objective\n",
    "\n",
    "Build a LangGraph workflow that **reads a customer review**, performs **sentiment analysis**, and conditionally creates a personalized response based on whether the sentiment is positive or negative. If the sentiment is negative, the workflow should **deeply analyze the review** before drafting the response.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Goals\n",
    "\n",
    "1. Accept a **review text** as input.\n",
    "2. Use an **LLM node** to:\n",
    "   - Determine sentiment: `positive` or `negative`\n",
    "   - Output structured JSON:  \n",
    "     ```json\n",
    "     { \"sentiment\": \"positive\" }\n",
    "     ```\n",
    "3. Based on sentiment, use **conditional routing**:\n",
    "   - If **positive** →  \n",
    "     → Go to **Response Generator Node** to generate a friendly reply  \n",
    "     → **End Flow**\n",
    "   - If **negative** →  \n",
    "     → Go to **Review Diagnostic Node**\n",
    "\n",
    "4. The **Review Diagnostic Node** (LLM):\n",
    "   - Extract 3 key insights from the review:\n",
    "     - `issue_type` (e.g., \"billing\", \"product defect\")\n",
    "     - `tone` (e.g., \"angry\", \"disappointed\")\n",
    "     - `urgency` (e.g., \"low\", \"medium\", \"high\")\n",
    "   - Output JSON:\n",
    "     ```json\n",
    "     {\n",
    "       \"issue_type\": \"delivery delay\",\n",
    "       \"tone\": \"frustrated\",\n",
    "       \"urgency\": \"high\"\n",
    "     }\n",
    "     ```\n",
    "\n",
    "5. Use these extracted insights to:\n",
    "   - Trigger a final **LLM Response Generator Node** that crafts an **empathetic and actionable reply**.\n",
    "\n",
    "6. End the workflow after sending the response.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Workflow Overview (Node Flow)\n",
    "\n",
    "1. **Input Node**  \n",
    "   - Receives the customer review text\n",
    "\n",
    "2. **Sentiment Analysis Node** (LLM)  \n",
    "   - Outputs sentiment in structured JSON\n",
    "\n",
    "3. **Conditional Router Node**  \n",
    "   - If `sentiment == \"positive\"` → `PositiveReplyNode`  \n",
    "   - If `sentiment == \"negative\"` → `DiagnosticNode`\n",
    "\n",
    "4. **DiagnosticNode** (LLM)  \n",
    "   - Extracts `issue_type`, `tone`, and `urgency`\n",
    "\n",
    "5. **NegativeReplyNode** (LLM)  \n",
    "   - Crafts customized reply based on diagnostic output\n",
    "\n",
    "6. **Output Node**  \n",
    "   - Returns the response\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Success Criteria\n",
    "\n",
    "- Accurate sentiment classification using LLM.\n",
    "- Structured JSON outputs at each step.\n",
    "- Intelligent routing via LangGraph's conditional edges.\n",
    "- Empathetic, situation-specific final responses.\n",
    "- Fully automated and adaptive workflow with minimal hardcoding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3ca73ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "565759cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2033dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSchema(BaseModel):\n",
    "    sentiment : Literal[\"Positive\", \"Negative\"] = Field(description=\"Sentiment of the review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4566786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisSchema(BaseModel):\n",
    "    issue_type: Literal[\"UX\", \"Performance\", \"Bug\", \"Support\", \"Other\"] = Field(description='The category of issue mentioned in the review')\n",
    "    tone: Literal[\"angry\", \"frustrated\", \"disappointed\", \"calm\"] = Field(description='The emotional tone expressed by the user')\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\"] = Field(description='How urgent or critical the issue appears to be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1ac6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model = model.with_structured_output(SentimentSchema)\n",
    "structured_model2 = model.with_structured_output(DiagnosisSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67eaecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \" What is the sentiment of the following review: the software is very good\"\n",
    "\n",
    "# structured_model.invoke(prompt).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5a07eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewState(TypedDict):\n",
    "    review : str\n",
    "    sentiment : Literal['Positive', \"Negative\"]\n",
    "    diagnosis : dict\n",
    "    response : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69c66989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment (state: ReviewState):\n",
    "    prompt = f'for the following the review find out the sentiment \\n {state['review']}'\n",
    "    sentiment = structured_model.invoke(prompt).sentiment\n",
    "\n",
    "    return {'sentiment': sentiment}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "426eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentiment(state:ReviewState) -> Literal[\"positive_response\",\"run_diagnosis\"]:\n",
    "    if state['sentiment'] ==\"Positive\":\n",
    "        return \"positive_response\"\n",
    "    else:\n",
    "        return \"run_diagnosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bd3ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_response(state:ReviewState):\n",
    "    prompt = f\"\"\"Write a warm thank-you message in response to this review:\n",
    "    \\n\\n\\\"{state['review']}\\\"\\n\n",
    "Also, kindly ask the user to leave feedback on our website.\"\"\"\n",
    "    \n",
    "    response = model.invoke(prompt).content\n",
    "\n",
    "    return {'response': response}\n",
    "\n",
    "\n",
    "def run_diagnosis(state: ReviewState):\n",
    "\n",
    "    prompt = f\"\"\"Diagnose this negative review:\\n\\n{state['review']}\\n\"\n",
    "    \"Return issue_type, tone, and urgency.\n",
    "\"\"\"\n",
    "    response = structured_model2.invoke(prompt)\n",
    "\n",
    "    return {'diagnosis': response.model_dump()}\n",
    "\n",
    "def negative_response(state:ReviewState):\n",
    "     diagnosis = state['diagnosis']\n",
    "     prompt = f\"\"\"You are a support assistant.\n",
    "The user had a '{diagnosis['issue_type']}' issue, sounded '{diagnosis['tone']}', and marked urgency as '{diagnosis['urgency']}'.\n",
    "Write an empathetic, helpful resolution message.\n",
    "\"\"\"\n",
    "     response = model.invoke(prompt).content\n",
    "\n",
    "     return {'response': response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29a41674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e0b882c9b0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(ReviewState)\n",
    "\n",
    "graph.add_node(\"find_sentiment\", find_sentiment)\n",
    "graph.add_node(\"positive_response\", positive_response)\n",
    "graph.add_node(\"run_diagnosis\", run_diagnosis)\n",
    "graph.add_node(\"negative_response\", negative_response)\n",
    "\n",
    "graph.add_conditional_edges('find_sentiment',check_sentiment)\n",
    "\n",
    "graph.add_edge('positive_response', END)\n",
    "\n",
    "graph.add_edge('run_diagnosis', 'negative_response')\n",
    "graph.add_edge('negative_response', END)\n",
    "\n",
    "graph.add_edge(START, \"find_sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f0455",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029fd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
