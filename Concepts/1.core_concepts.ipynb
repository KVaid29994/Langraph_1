{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2dc4015",
   "metadata": {},
   "source": [
    "## LLM Workflows:\n",
    "\n",
    "\n",
    "Large Language Model (LLM) workflows define how to effectively use LLMs in applications—from input formulation to output post-processing. A typical LLM pipeline includes several modular stages to ensure performance, relevance, and accuracy.\n",
    "\n",
    "## 🛠️ Key Components of LLM Workflow\n",
    "\n",
    "### 1. **Prompt Engineering**\n",
    "- Crafting the input to guide the LLM effectively.\n",
    "- Techniques:\n",
    "  - Zero-shot / One-shot / Few-shot prompting\n",
    "  - Instruction tuning\n",
    "  - Role prompting (e.g., \"You are an expert...\")\n",
    "\n",
    "### 2. **Pre-processing**\n",
    "- Cleaning and formatting the input text.\n",
    "- Tokenization if using low-level model APIs.\n",
    "- Adding context, metadata, or constraints.\n",
    "\n",
    "### 3. **Model Invocation**\n",
    "- Calling the LLM (e.g., GPT-4, Claude, Mistral).\n",
    "- Options:\n",
    "  - Cloud APIs (OpenAI, Anthropic, Cohere)\n",
    "  - Local deployment (LLaMA, Mistral, Falcon)\n",
    "\n",
    "### 4. **Post-processing**\n",
    "- Structuring output: JSON, SQL, natural language, etc.\n",
    "- Validation: checking format, safety, hallucination.\n",
    "- Ranking / filtering / rephrasing if needed.\n",
    "\n",
    "### 5. **Memory Management**\n",
    "- Keeping track of previous interactions.\n",
    "- Tools:\n",
    "  - Vector stores (e.g., FAISS, Chroma)\n",
    "  - Memory modules in LangChain, LlamaIndex\n",
    "\n",
    "### 6. **Retrieval-Augmented Generation (RAG)**\n",
    "- Querying external data sources (DBs, documents).\n",
    "- Injecting retrieved info into the prompt.\n",
    "- Enhances accuracy and domain specificity.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Advanced Workflows\n",
    "\n",
    "### 🔹 Agent-based Workflows\n",
    "- Use of autonomous agents to reason and act step-by-step.\n",
    "- Libraries: LangChain Agents, CrewAI, AutoGen.\n",
    "\n",
    "### 🔹 Tool Use / Function Calling\n",
    "- LLM can call external tools (API, calculator, DB).\n",
    "- Format: OpenAI tool calling / Function calling schema.\n",
    "\n",
    "### 🔹 Chain-of-Thought Reasoning (CoT)\n",
    "- Explicit reasoning steps to arrive at answers.\n",
    "- Useful for logical, mathematical, or planning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Evaluation Techniques\n",
    "\n",
    "### 🔸 Intrinsic Metrics\n",
    "- Perplexity\n",
    "- Log-likelihood\n",
    "- Token usage / latency\n",
    "\n",
    "### 🔸 Extrinsic Metrics\n",
    "- Task-specific accuracy (e.g., QA accuracy)\n",
    "- Human evaluation (fluency, helpfulness)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Libraries & Frameworks\n",
    "\n",
    "- **LangChain**: Chains, agents, memory, RAG, tools.\n",
    "- **LlamaIndex**: Document loaders, indexes, RAG support.\n",
    "- **CrewAI**: Multi-agent workflow orchestration.\n",
    "- **Haystack**: End-to-end NLP pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Security & Safety Considerations\n",
    "\n",
    "- Input sanitization\n",
    "- Prompt injection detection/prevention\n",
    "- Output moderation and red teaming\n",
    "- Usage logging and monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Example Use Case Flow (RAG + Tools)\n",
    "\n",
    "1. **User Input** →  \n",
    "2. **Retrieve relevant docs** (RAG) →  \n",
    "3. **Construct prompt** with context →  \n",
    "4. **Call LLM** →  \n",
    "5. **If tool needed**, trigger tool →  \n",
    "6. **Post-process result** →  \n",
    "7. **Return response**\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770275e",
   "metadata": {},
   "source": [
    "# 🌐 LangGraph Workflows Explained\n",
    "\n",
    "LangGraph allows you to build **multi-agent** or **multi-step** LLM applications using a **graph-based architecture**. Each node can be a tool, an agent, or an LLM call, and edges define how data flows.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Workflow Types in LangGraph\n",
    "\n",
    "LangGraph supports several types of execution workflows, depending on how you want nodes (or steps) to interact and run.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧵 1. **Sequential Workflow**\n",
    "\n",
    "In this flow, nodes are executed **one after another**, just like a pipeline.\n",
    "\n",
    "```text\n",
    "Start ➡️ Step 1 ➡️ Step 2 ➡️ Step 3 ➡️ End\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "### ⚡ 2. Parallel Workflow (Fan-Out / Fan-In)\n",
    "\n",
    "Multiple nodes are run at the same time, and their outputs are later combined.\n",
    "\n",
    "```text\n",
    "         ➡️ Step A ➡️ \n",
    "Start ➡️             ➡️ Merge ➡️ End\n",
    "         ➡️ Step B ➡️\n",
    "\n",
    "```\n",
    "\n",
    ">> Use await_all to wait for all branches before proceeding.\n",
    "---\n",
    "\n",
    "### 🔁 3. Looping Workflow (Iteration / Feedback Loops)\n",
    "\n",
    "A node or set of nodes is executed repeatedly based on a condition.\n",
    "\n",
    "```text\n",
    "Step 1 🔁 Step 2 🔁 Step 3 🔁 ... until done\n",
    "```\n",
    "\n",
    "- 🔍 Ideal for refinement, self-correction, or reflection loops\n",
    "\n",
    "- 👀 Use cases: summarization refinement, multi-pass question answering\n",
    "\n",
    "---\n",
    "\n",
    "### 🤹 4. Conditional Workflow (Branching)\n",
    "\n",
    "> Flow changes dynamically based on conditions or data.\n",
    "\n",
    "```text\n",
    "\n",
    "      ➡️ Step A (if condition)\n",
    "Start \n",
    "      ➡️ Step B (else) ➡️ End\n",
    "\n",
    "```\n",
    "\n",
    "- 🔄 Allows dynamic routing\n",
    "\n",
    "- 💡 Useful for tools like: \"is this a search query?\" → yes → search agent, else → answer directly\"\n",
    "\n",
    "---\n",
    "\n",
    "### 🕸️ 5. Graph-of-Graphs (Nested Workflows)\n",
    "\n",
    "One node can itself be a subgraph, representing a more complex flow.\n",
    "\n",
    "```text\n",
    "\n",
    "Start ➡️ Subgraph A ➡️ Step X ➡️ End\n",
    "\n",
    "```\n",
    "- 🧱 Modular and reusable logic\n",
    "- 🧠 Great for task decomposition or agent planning\n",
    "\n",
    "### 🧠 6. Multi-Agent Orchestration\n",
    "---\n",
    "\n",
    "Each node is an agent, and you can route tasks based on capabilities.\n",
    "\n",
    "``` text\n",
    "\n",
    "Start ➡️ Classifier ➡️ Agent A / Agent B ➡️ Final Merge\n",
    "```\n",
    "= 🤖 For specialist agents (e.g., code, math, search)\n",
    "\n",
    "- 🤝 Often used with CrewAI or LangChain Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7942438",
   "metadata": {},
   "source": [
    "| Workflow Type   | Use Case                       | Parallel?      |\n",
    "| --------------- | ------------------------------ | -------------- |\n",
    "| Sequential      | Simple pipelines               | ❌              |\n",
    "| Parallel        | Independent tasks              | ✅              |\n",
    "| Looping         | Feedback, iterative refinement | ⚠️ Conditional |\n",
    "| Conditional     | Rule-based routing             | ❌              |\n",
    "| Graph-of-Graphs | Modular flows                  | Depends        |\n",
    "| Multi-Agent     | Agent-based problem solving    | ✅              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbd111",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dabc7",
   "metadata": {},
   "source": [
    "## 🧠 LangGraph: Graph, Nodes & Edges Explained\n",
    "\n",
    "LangGraph is a framework that helps you build **LLM-based workflows** using a **graph structure**. These workflows are modular, composable, and interpretable — making it easy to orchestrate complex interactions between LLMs, tools, and agents.\n",
    "\n",
    "---\n",
    "### 📊 What is a Graph?\n",
    "\n",
    "- A **Graph** is a structured representation of a workflow.\n",
    "- It defines the **flow of execution** — what steps happen and in what order.\n",
    "- Think of it as a **blueprint of your AI process**, where each step is a **node**, and the paths between them are **edges**.\n",
    "- LangGraph leverages **directed graphs**, meaning the execution flows from one node to the next in a specific direction.\n",
    "\n",
    "📌 **In LangGraph:**\n",
    "- The graph is your entire LLM application flow.\n",
    "- It maps **how data moves**, **what is computed**, and **when**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 What is a Node?\n",
    "\n",
    "- A **Node** is a **unit of computation** in the graph.\n",
    "- It usually wraps a **Python function**, a **tool call**, or a **language model invocation**.\n",
    "- Nodes can perform a variety of tasks:\n",
    "  - Calling an LLM with a prompt\n",
    "  - Running a tool like a calculator or search API\n",
    "  - Executing custom business logic\n",
    "- Each node can have **input** and **output**, which can be passed to other nodes via edges.\n",
    "\n",
    "💡 **Think of a node as a building block** — it does something useful and passes its result to the next step.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 What is an Edge?\n",
    "\n",
    "- An **Edge** defines the **connection between two nodes**.\n",
    "- It tells the graph **what should happen next** after a node finishes execution.\n",
    "- Edges control the **data flow and execution logic** — whether the flow is sequential, conditional, parallel, or iterative.\n",
    "\n",
    "📌 Types of flows defined by edges:\n",
    "- **Sequential**: Step A → Step B → Step C\n",
    "- **Parallel**: One step leads to multiple nodes that run in parallel\n",
    "- **Conditional**: Node output determines which path (edge) to follow next\n",
    "- **Looping**: Edges that bring the execution back to a previous node\n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 Why This Matters?\n",
    "\n",
    "LangGraph's graph-node-edge architecture provides:\n",
    "\n",
    "- 🔄 **Flexibility**: Easily define branching, loops, and re-routing\n",
    "- 🧱 **Modularity**: Reuse nodes in different graphs or workflows\n",
    "- 📈 **Transparency**: Visualize and debug the workflow\n",
    "- ⚡ **Efficiency**: Enable parallelism and optimized execution\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Graph** | The overall workflow of your LLM application |\n",
    "| **Node** | A functional unit that performs a task (e.g., calls an LLM, runs logic) |\n",
    "| **Edge** | Defines the flow from one node to another; controls logic, branching, and sequencing |\n",
    "\n",
    "🚀 **LangGraph helps you build smart, structured LLM apps that scale.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d650f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2760cd",
   "metadata": {},
   "source": [
    "## 🧠 The Concept of State in LangGraph\n",
    "\n",
    "In any LLM-based workflow, **data plays a central role** in guiding how the workflow proceeds. This evolving data — the one that flows between steps and gets updated — is referred to as the **State**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 What is \"State\"?\n",
    "\n",
    "- The **State** is a **data object** that stores all relevant information required by the LLM or tools throughout the execution of a LangGraph.\n",
    "- It acts as a **memory** of the system that holds:\n",
    "  - User inputs\n",
    "  - Intermediate results\n",
    "  - Model responses\n",
    "  - Flags or signals for control flow\n",
    "\n",
    "🧭 **In simple terms**:  \n",
    "> _The State is the container that carries and updates the data as it flows through the graph._\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Why State is Important\n",
    "\n",
    "- LLMs are **stateless** by default — they don’t retain memory between calls.\n",
    "- Workflows, however, require context to make decisions, reuse information, or reflect changes — that's where **state comes in**.\n",
    "- It enables:\n",
    "  - Tracking progress in a multi-step process\n",
    "  - Storing results from one node to be used in another\n",
    "  - Enabling dynamic branching, condition checks, and looping\n",
    "\n",
    "---\n",
    "### 📈 How State Evolves\n",
    "\n",
    "At each step (node), the state can be:\n",
    "- 🔍 **Read**: Access input data, previous results, user queries\n",
    "- ✍️ **Updated**: Add a new key-value pair or modify existing data\n",
    "- 🔁 **Passed forward**: Shared with the next node(s) in the workflow\n",
    "\n",
    "Every time a node executes:\n",
    "- It receives the **current state**\n",
    "- Performs its logic\n",
    "- Returns an **updated version of the state**\n",
    "\n",
    "Thus, the **state evolves dynamically** as the workflow progresses.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 What Can Be Stored in State?\n",
    "\n",
    "You can store almost anything that makes sense for the task:\n",
    "\n",
    "| Type of Data | Examples |\n",
    "|--------------|----------|\n",
    "| 🔤 Text       | User prompts, LLM outputs |\n",
    "| 🔢 Numbers    | Scores, counts, confidence levels |\n",
    "| 📄 Objects    | JSON, dictionaries, documents |\n",
    "| 🔁 Flags      | Booleans to control flow (e.g., `should_retry: True`) |\n",
    "| 🧠 History    | Conversation history, prior steps, actions taken |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **State** | The evolving data that flows through the graph and guides execution |\n",
    "| **Purpose** | Stores context, tracks results, and controls flow |\n",
    "| **Behavior** | Gets read, updated, and passed forward at each node |\n",
    "\n",
    "🧠 **State = the memory of your LangGraph. It holds everything your LLM agents need to think, decide, and act.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f944548",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb931a21",
   "metadata": {},
   "source": [
    "## 🔁 The Concept of Reducers in LangGraph\n",
    "\n",
    "In LangGraph, the **State** is shared and accessible by all nodes. Since this state is **mutable** and updated by multiple nodes, it's important to define **how exactly** those updates should be applied.\n",
    "\n",
    "That’s where **Reducers** come into play! 🧩\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What is a Reducer?\n",
    "\n",
    "- A **Reducer** is a function or rule that determines **how new data is applied to an existing state** key.\n",
    "- It acts like a policy that says:\n",
    "  > _\"When new data arrives at this key, should I replace the old value? Merge with it? Append to it?\"_\n",
    "\n",
    "Think of it as a **smart update mechanism**.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Why Reducers are Needed\n",
    "\n",
    "- Multiple nodes may update the same key in the state.\n",
    "- Without rules, updates could **overwrite each other**, cause **data loss**, or create **inconsistent state**.\n",
    "- Reducers give you **control over how state evolves** — ensuring correctness, traceability, and logic.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 Per-Key Reducers\n",
    "\n",
    "- Each key in the state can have its **own reducer**.\n",
    "- This allows fine-grained control — you can customize behavior for each type of data.\n",
    "\n",
    "📌 For example:\n",
    "| Key in State      | Reducer Behavior                   |\n",
    "|-------------------|------------------------------------|\n",
    "| `messages`        | Append to existing list            |\n",
    "| `final_answer`    | Replace with new output            |\n",
    "| `search_results`  | Merge new results with existing    |\n",
    "| `errors`          | Add to error log list              |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Common Reducer Strategies\n",
    "\n",
    "| Strategy     | Description |\n",
    "|--------------|-------------|\n",
    "| **Replace**  | New value **overwrites** the old one (default behavior) |\n",
    "| **Append**   | New value is **added** to an existing list |\n",
    "| **Merge**    | Combines dictionaries or data structures |\n",
    "| **Custom**   | Define your own logic based on context |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "| Concept   | Description |\n",
    "|-----------|-------------|\n",
    "| **Reducer** | A function that defines **how a key in the state is updated** when a node returns new data |\n",
    "| **Purpose** | Ensures predictable and conflict-free updates to shared state |\n",
    "| **Flexibility** | Each key in the state can have its **own reducer strategy** |\n",
    "\n",
    "🧠 **Reducers = Traffic controllers for state updates**  \n",
    "They make sure all updates are applied logically and consistently! 🛣️\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5897e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89aec11",
   "metadata": {},
   "source": [
    "## ⚙️ LangGraph Execution Model\n",
    "\n",
    "LangGraph’s execution model is **inspired by Google Pregel**, a framework built for **large-scale graph processing**.  \n",
    "It follows a **message-passing and super-step based execution** strategy — making it powerful for orchestrating complex, dynamic LLM workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 Overview: Execution in 6 Stages\n",
    "\n",
    "LangGraph runs in structured **rounds (super-steps)** where nodes update state and pass messages.  \n",
    "Let’s break it down step-by-step:\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Graph Definition\n",
    "\n",
    "- First, define the **structure** of your application:\n",
    "  - Nodes (functions, tools, LLM calls)\n",
    "  - Edges (execution flow)\n",
    "  - State schema (what data is shared and how it evolves)\n",
    "\n",
    "🔧 This is like building the blueprint of your workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Compilation\n",
    "\n",
    "- The defined graph is **compiled** to:\n",
    "  - Verify the structure\n",
    "  - Check for logical correctness\n",
    "  - Ensure all nodes and edges are valid\n",
    "\n",
    "🧪 Think of this as a **pre-flight check** to catch errors early.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Invocation\n",
    "\n",
    "- You invoke the graph using `.invoke(initial_state)`\n",
    "- The **initial state** is passed to the **starting node**\n",
    "- This node performs computation and makes a **partial update** to the state\n",
    "\n",
    "🚀 This kicks off the entire graph execution.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Super-Steps Begin\n",
    "\n",
    "- The execution moves in **rounds** (called **super-steps**)\n",
    "- In each round:\n",
    "  - Active nodes receive the current state\n",
    "  - They **process**, **update state**, and **send messages** to other nodes\n",
    "\n",
    "🔁 Super-steps continue until the graph naturally converges.\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Message Passing & Node Activation\n",
    "\n",
    "- Nodes communicate by **sending messages**\n",
    "- A node is **activated** when:\n",
    "  - It receives a message\n",
    "  - Or it's scheduled by an edge from an active node\n",
    "- Activated nodes participate in the next super-step\n",
    "\n",
    "📬 This is similar to **distributed parallel computing**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6️⃣ When Does Execution Stop?\n",
    "\n",
    "LangGraph execution **terminates** when:\n",
    "\n",
    "✅ **No nodes are active**  \n",
    "✅ **No messages are in transit**\n",
    "\n",
    "📴 This ensures the workflow has fully completed and there's nothing left to compute.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "| Phase            | What Happens                                          |\n",
    "|------------------|-------------------------------------------------------|\n",
    "| Graph Definition | Nodes, edges, and state schema are defined            |\n",
    "| Compilation      | Structural and logical validation                     |\n",
    "| Invocation       | Initial state passed to first node via `.invoke()`    |\n",
    "| Super-Steps      | Workflow runs in rounds, nodes act in parallel        |\n",
    "| Message Passing  | Nodes trigger others through message-driven logic     |\n",
    "| Termination      | Stops when no nodes or messages are left              |\n",
    "\n",
    "---\n",
    "\n",
    "🧠 **LangGraph = Scalable + Parallel + Deterministic Execution of LLM Graphs**\n",
    "\n",
    "Inspired by Pregel, optimized for **LLM workflows** and **multi-agent systems**! 🚀\n",
    "Let me know if you'd like a visual flowchart version of this or an example showing super-step flow in a real application!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
